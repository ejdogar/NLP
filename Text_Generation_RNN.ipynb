{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejdogar/NLP/blob/main/Text_Generation_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import keras"
      ],
      "metadata": {
        "id": "ddOoR2n_tbcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3SjHAwwO6f"
      },
      "outputs": [],
      "source": [
        "docs = [\"Hey how are you\",\n",
        "        \"Hey whats up\",\n",
        "        \"Pakistan lost the match\",\n",
        "        \"it is raining\",\n",
        "        \"Charles Leclerc will be the F1 champion of 2024\",\n",
        "        \"Ronaldo is the best football player\",\n",
        "        \"Schumacher was the best driver ever\",\n",
        "        \"Lewis Hamilton is a cheat\",\n",
        "        \"Sebastian Vettel is struggling\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "YwzJ82CHovWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "Cp03Z5QEwkXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(docs)"
      ],
      "metadata": {
        "id": "HMrkx3QCo8XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3xrCVbBuBV4",
        "outputId": "b285e514-290f-4bee-9fc6-706fb1273cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 5, 6, 7], [3, 8, 9], [10, 11, 1, 12], [13, 2, 14], [15, 16, 17, 18, 1, 19, 20, 21, 22], [23, 2, 1, 4, 24, 25], [26, 27, 1, 4, 28, 29], [30, 31, 2, 32, 33], [34, 35, 2, 36]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2int = tokenizer.word_index\n",
        "print(word2int)\n",
        "vocab_size = len(word2int)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZZU7pxvpMBU",
        "outputId": "d774088e-6bde-42c8-f1a9-142ae201b0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'is': 2, 'hey': 3, 'best': 4, 'how': 5, 'are': 6, 'you': 7, 'whats': 8, 'up': 9, 'pakistan': 10, 'lost': 11, 'match': 12, 'it': 13, 'raining': 14, 'charles': 15, 'leclerc': 16, 'will': 17, 'be': 18, 'f1': 19, 'champion': 20, 'of': 21, '2024': 22, 'ronaldo': 23, 'football': 24, 'player': 25, 'schumacher': 26, 'was': 27, 'driver': 28, 'ever': 29, 'lewis': 30, 'hamilton': 31, 'a': 32, 'cheat': 33, 'sebastian': 34, 'vettel': 35, 'struggling': 36}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int2word = {idx+1: word for idx, word in enumerate(word2int)}"
      ],
      "metadata": {
        "id": "R6VCKKAbqeFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "sequences = pad_sequences(sequences, padding=\"post\")\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53QDxLVaq9KI",
        "outputId": "d425a48b-ddf0-47e2-fb3b-23426118c5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  5,  6,  7,  0,  0,  0,  0,  0],\n",
              "       [ 3,  8,  9,  0,  0,  0,  0,  0,  0],\n",
              "       [10, 11,  1, 12,  0,  0,  0,  0,  0],\n",
              "       [13,  2, 14,  0,  0,  0,  0,  0,  0],\n",
              "       [15, 16, 17, 18,  1, 19, 20, 21, 22],\n",
              "       [23,  2,  1,  4, 24, 25,  0,  0,  0],\n",
              "       [26, 27,  1,  4, 28, 29,  0,  0,  0],\n",
              "       [30, 31,  2, 32, 33,  0,  0,  0,  0],\n",
              "       [34, 35,  2, 36,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlength = len(sequences[0])\n",
        "maxlength"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldIIfzbrr31n",
        "outputId": "838b482a-a490-4a09-da6a-637a122567e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = []\n",
        "target_seq = []\n",
        "for i in range(len(sequences)):\n",
        "  input_seq.append(sequences[i][:-1])\n",
        "  target_seq.append(sequences[i][1:])\n",
        "\n",
        "print(f\"{input_seq[0]=}\")\n",
        "print(f\"{target_seq[0]=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNVe0ZHGsEFN",
        "outputId": "a03c55a1-6e89-499f-841f-8cf44bc37bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_seq[0]=array([3, 5, 6, 7, 0, 0, 0, 0], dtype=int32)\n",
            "target_seq[0]=array([5, 6, 7, 0, 0, 0, 0, 0], dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = maxlength - 1\n",
        "def one_hot_encoder(sequence, batch_size, seq_len, vocab_size):\n",
        "  feature = np.zeros((batch_size, seq_len, vocab_size+1), dtype = np.float32)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    for k in range(seq_len):\n",
        "      feature[i, k, sequence[i][k]] = 1\n",
        "\n",
        "  return feature"
      ],
      "metadata": {
        "id": "kpJL1UW7szrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = len(docs)\n",
        "input_seq = one_hot_encoder(input_seq, batch_size, seq_len, vocab_size)"
      ],
      "metadata": {
        "id": "0rnK_p3_t5nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ],
      "metadata": {
        "id": "op2wiJenzhfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU is available\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU not available, running on CPU...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sIjpCrzzuzR",
        "outputId": "4d0f14e2-008e-47e5-e6aa-be2d2bff2fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, running on CPU...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first = True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "    out = out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "qYdksuRj0Viy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(input_size = vocab_size+1, output_size = vocab_size+1, hidden_dim = 8, n_layers = 1)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "vzzRi4Pz3D2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 7000\n",
        "lr = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "q38pSa0K3xZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = input_seq.to(device)\n",
        "print(f\"{input_seq.shape=}\")\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  out, hidden = model(input_seq)\n",
        "  out.to(device)\n",
        "  target_seq.to(device)\n",
        "  loss = criterion(out, target_seq.view(-1).long())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%50  == 0:\n",
        "    print('Epoch: {}/{}......'.format(epoch, n_epochs), end=' ')\n",
        "    print(\"Loss : {:.4f}\".format(loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpI2O_4E4HBh",
        "outputId": "478aafb5-a949-4a51-dcd8-dfaf58d67577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_seq.shape=torch.Size([9, 8, 37])\n",
            "Epoch: 50/7000...... Loss : 3.7481\n",
            "Epoch: 100/7000...... Loss : 3.7205\n",
            "Epoch: 150/7000...... Loss : 3.6937\n",
            "Epoch: 200/7000...... Loss : 3.6670\n",
            "Epoch: 250/7000...... Loss : 3.6392\n",
            "Epoch: 300/7000...... Loss : 3.6096\n",
            "Epoch: 350/7000...... Loss : 3.5770\n",
            "Epoch: 400/7000...... Loss : 3.5406\n",
            "Epoch: 450/7000...... Loss : 3.4996\n",
            "Epoch: 500/7000...... Loss : 3.4533\n",
            "Epoch: 550/7000...... Loss : 3.4014\n",
            "Epoch: 600/7000...... Loss : 3.3436\n",
            "Epoch: 650/7000...... Loss : 3.2803\n",
            "Epoch: 700/7000...... Loss : 3.2117\n",
            "Epoch: 750/7000...... Loss : 3.1383\n",
            "Epoch: 800/7000...... Loss : 3.0608\n",
            "Epoch: 850/7000...... Loss : 2.9806\n",
            "Epoch: 900/7000...... Loss : 2.8997\n",
            "Epoch: 950/7000...... Loss : 2.8206\n",
            "Epoch: 1000/7000...... Loss : 2.7453\n",
            "Epoch: 1050/7000...... Loss : 2.6752\n",
            "Epoch: 1100/7000...... Loss : 2.6109\n",
            "Epoch: 1150/7000...... Loss : 2.5526\n",
            "Epoch: 1200/7000...... Loss : 2.4999\n",
            "Epoch: 1250/7000...... Loss : 2.4525\n",
            "Epoch: 1300/7000...... Loss : 2.4099\n",
            "Epoch: 1350/7000...... Loss : 2.3714\n",
            "Epoch: 1400/7000...... Loss : 2.3366\n",
            "Epoch: 1450/7000...... Loss : 2.3049\n",
            "Epoch: 1500/7000...... Loss : 2.2761\n",
            "Epoch: 1550/7000...... Loss : 2.2495\n",
            "Epoch: 1600/7000...... Loss : 2.2250\n",
            "Epoch: 1650/7000...... Loss : 2.2022\n",
            "Epoch: 1700/7000...... Loss : 2.1807\n",
            "Epoch: 1750/7000...... Loss : 2.1603\n",
            "Epoch: 1800/7000...... Loss : 2.1408\n",
            "Epoch: 1850/7000...... Loss : 2.1220\n",
            "Epoch: 1900/7000...... Loss : 2.1036\n",
            "Epoch: 1950/7000...... Loss : 2.0856\n",
            "Epoch: 2000/7000...... Loss : 2.0679\n",
            "Epoch: 2050/7000...... Loss : 2.0503\n",
            "Epoch: 2100/7000...... Loss : 2.0328\n",
            "Epoch: 2150/7000...... Loss : 2.0155\n",
            "Epoch: 2200/7000...... Loss : 1.9982\n",
            "Epoch: 2250/7000...... Loss : 1.9808\n",
            "Epoch: 2300/7000...... Loss : 1.9634\n",
            "Epoch: 2350/7000...... Loss : 1.9458\n",
            "Epoch: 2400/7000...... Loss : 1.9279\n",
            "Epoch: 2450/7000...... Loss : 1.9097\n",
            "Epoch: 2500/7000...... Loss : 1.8908\n",
            "Epoch: 2550/7000...... Loss : 1.8712\n",
            "Epoch: 2600/7000...... Loss : 1.8502\n",
            "Epoch: 2650/7000...... Loss : 1.8259\n",
            "Epoch: 2700/7000...... Loss : 1.7971\n",
            "Epoch: 2750/7000...... Loss : 1.7744\n",
            "Epoch: 2800/7000...... Loss : 1.7540\n",
            "Epoch: 2850/7000...... Loss : 1.7346\n",
            "Epoch: 2900/7000...... Loss : 1.7158\n",
            "Epoch: 2950/7000...... Loss : 1.6975\n",
            "Epoch: 3000/7000...... Loss : 1.6797\n",
            "Epoch: 3050/7000...... Loss : 1.6623\n",
            "Epoch: 3100/7000...... Loss : 1.6453\n",
            "Epoch: 3150/7000...... Loss : 1.6285\n",
            "Epoch: 3200/7000...... Loss : 1.6121\n",
            "Epoch: 3250/7000...... Loss : 1.5958\n",
            "Epoch: 3300/7000...... Loss : 1.5798\n",
            "Epoch: 3350/7000...... Loss : 1.5640\n",
            "Epoch: 3400/7000...... Loss : 1.5483\n",
            "Epoch: 3450/7000...... Loss : 1.5328\n",
            "Epoch: 3500/7000...... Loss : 1.5174\n",
            "Epoch: 3550/7000...... Loss : 1.5021\n",
            "Epoch: 3600/7000...... Loss : 1.4869\n",
            "Epoch: 3650/7000...... Loss : 1.4718\n",
            "Epoch: 3700/7000...... Loss : 1.4567\n",
            "Epoch: 3750/7000...... Loss : 1.4418\n",
            "Epoch: 3800/7000...... Loss : 1.4269\n",
            "Epoch: 3850/7000...... Loss : 1.4120\n",
            "Epoch: 3900/7000...... Loss : 1.3972\n",
            "Epoch: 3950/7000...... Loss : 1.3825\n",
            "Epoch: 4000/7000...... Loss : 1.3678\n",
            "Epoch: 4050/7000...... Loss : 1.3532\n",
            "Epoch: 4100/7000...... Loss : 1.3386\n",
            "Epoch: 4150/7000...... Loss : 1.3241\n",
            "Epoch: 4200/7000...... Loss : 1.3096\n",
            "Epoch: 4250/7000...... Loss : 1.2951\n",
            "Epoch: 4300/7000...... Loss : 1.2808\n",
            "Epoch: 4350/7000...... Loss : 1.2665\n",
            "Epoch: 4400/7000...... Loss : 1.2522\n",
            "Epoch: 4450/7000...... Loss : 1.2380\n",
            "Epoch: 4500/7000...... Loss : 1.2239\n",
            "Epoch: 4550/7000...... Loss : 1.2099\n",
            "Epoch: 4600/7000...... Loss : 1.1959\n",
            "Epoch: 4650/7000...... Loss : 1.1821\n",
            "Epoch: 4700/7000...... Loss : 1.1683\n",
            "Epoch: 4750/7000...... Loss : 1.1546\n",
            "Epoch: 4800/7000...... Loss : 1.1410\n",
            "Epoch: 4850/7000...... Loss : 1.1276\n",
            "Epoch: 4900/7000...... Loss : 1.1142\n",
            "Epoch: 4950/7000...... Loss : 1.1009\n",
            "Epoch: 5000/7000...... Loss : 1.0877\n",
            "Epoch: 5050/7000...... Loss : 1.0746\n",
            "Epoch: 5100/7000...... Loss : 1.0617\n",
            "Epoch: 5150/7000...... Loss : 1.0488\n",
            "Epoch: 5200/7000...... Loss : 1.0361\n",
            "Epoch: 5250/7000...... Loss : 1.0234\n",
            "Epoch: 5300/7000...... Loss : 1.0109\n",
            "Epoch: 5350/7000...... Loss : 0.9985\n",
            "Epoch: 5400/7000...... Loss : 0.9861\n",
            "Epoch: 5450/7000...... Loss : 0.9739\n",
            "Epoch: 5500/7000...... Loss : 0.9618\n",
            "Epoch: 5550/7000...... Loss : 0.9499\n",
            "Epoch: 5600/7000...... Loss : 0.9380\n",
            "Epoch: 5650/7000...... Loss : 0.9262\n",
            "Epoch: 5700/7000...... Loss : 0.9146\n",
            "Epoch: 5750/7000...... Loss : 0.9031\n",
            "Epoch: 5800/7000...... Loss : 0.8917\n",
            "Epoch: 5850/7000...... Loss : 0.8804\n",
            "Epoch: 5900/7000...... Loss : 0.8692\n",
            "Epoch: 5950/7000...... Loss : 0.8581\n",
            "Epoch: 6000/7000...... Loss : 0.8471\n",
            "Epoch: 6050/7000...... Loss : 0.8363\n",
            "Epoch: 6100/7000...... Loss : 0.8255\n",
            "Epoch: 6150/7000...... Loss : 0.8149\n",
            "Epoch: 6200/7000...... Loss : 0.8043\n",
            "Epoch: 6250/7000...... Loss : 0.7939\n",
            "Epoch: 6300/7000...... Loss : 0.7836\n",
            "Epoch: 6350/7000...... Loss : 0.7733\n",
            "Epoch: 6400/7000...... Loss : 0.7632\n",
            "Epoch: 6450/7000...... Loss : 0.7532\n",
            "Epoch: 6500/7000...... Loss : 0.7433\n",
            "Epoch: 6550/7000...... Loss : 0.7335\n",
            "Epoch: 6600/7000...... Loss : 0.7238\n",
            "Epoch: 6650/7000...... Loss : 0.7142\n",
            "Epoch: 6700/7000...... Loss : 0.7047\n",
            "Epoch: 6750/7000...... Loss : 0.6952\n",
            "Epoch: 6800/7000...... Loss : 0.6859\n",
            "Epoch: 6850/7000...... Loss : 0.6767\n",
            "Epoch: 6900/7000...... Loss : 0.6676\n",
            "Epoch: 6950/7000...... Loss : 0.6586\n",
            "Epoch: 7000/7000...... Loss : 0.6497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, words):\n",
        "  sentence = np.array([[word2int[word] for word in words]])\n",
        "  sentence = one_hot_encoder(sentence, 1, sentence.shape[1], vocab_size)\n",
        "  sentence = torch.from_numpy(sentence)\n",
        "  sentence = sentence.to(device)\n",
        "\n",
        "  out, hidden = model(sentence)\n",
        "\n",
        "  prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "  word_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "  return int2word[word_ind+1], hidden"
      ],
      "metadata": {
        "id": "R1Wrblx7HmYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, out_size, start = \"hey\"):\n",
        "  model.eval()\n",
        "\n",
        "  start = start.lower()\n",
        "\n",
        "  start = start.split()\n",
        "\n",
        "  words = [word for word in start]\n",
        "\n",
        "  size = out_size - len(words)\n",
        "\n",
        "  print(\"===============\",size)\n",
        "  for i in range(size):\n",
        "    o, h = predict(model, words)\n",
        "    words.append(o)\n",
        "\n",
        "  return \" \".join(words)"
      ],
      "metadata": {
        "id": "93FD5LukJ9j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample(model, 7, \"is\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wQr4U7xfLZ7y",
        "outputId": "c2f467b5-053f-4768-e141-f1c69a3aaf5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is is is will f1 a sebastian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1imo2U5W9AzAqjxvNIqe2GhlEx23VLftA",
      "authorship_tag": "ABX9TyNwd1kP1EsNOcvCKz1HHsKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}